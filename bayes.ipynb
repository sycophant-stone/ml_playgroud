{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n"
     ]
    }
   ],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec=[0,1,0,1,0,1] # 1: 具有侮辱性词汇 0: 正常的\n",
    "    return postingList,classVec\n",
    "postingList,classVec=loadDataSet()\n",
    "print(postingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'posting', 'cute', 'love', 'garbage', 'licks', 'quit', 'not', 'food', 'help', 'stupid', 'park', 'dalmation', 'take', 'my', 'is', 'maybe', 'so', 'I', 'worthless', 'mr', 'steak', 'problems', 'dog', 'flea', 'please', 'ate', 'has', 'buying', 'to', 'him', 'stop']\n"
     ]
    }
   ],
   "source": [
    "# 根据dataset,去掉重复字段,创建一个词汇表.\n",
    "def createVocabList(dataset):\n",
    "    vocabSet=set([]) \n",
    "    for document in dataset:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    return list(vocabSet)\n",
    "VocabOfpostingList=createVocabList(postingList)\n",
    "print(VocabOfpostingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    # 看inputSet中的单词在我们的词汇表中有没有,如果没有的话,做个标注.返回log\n",
    "    returnVec=[0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]=1\n",
    "        else:\n",
    "            print(\"the word:%s is not in my Vocabulary!\"%word)\n",
    "    return returnVec\n",
    "\n",
    "vocab0=setOfWords2Vec(VocabOfpostingList,postingList[0])# 第0条留言出现了哪些词.其实是用vacabList这种方式描述了这个词分布.\n",
    "vocab1=setOfWords2Vec(VocabOfpostingList,postingList[1])# 第1条留言出现了哪些词.其实是用vacabList这种方式描述了这个词分布.\n",
    "print(vocab0)\n",
    "print(vocab1)\n",
    "print(len(vocab0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]]\n",
      "[-2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -3.25809654 -3.25809654 -3.25809654 -2.56494936 -3.25809654 -3.25809654\n",
      " -2.56494936 -3.25809654 -1.87180218 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -2.56494936\n",
      " -2.56494936 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.15948425 -2.56494936]\n",
      "[-3.04452244 -2.35137526 -3.04452244 -3.04452244 -2.35137526 -3.04452244\n",
      " -2.35137526 -2.35137526 -2.35137526 -3.04452244 -1.65822808 -2.35137526\n",
      " -3.04452244 -2.35137526 -3.04452244 -3.04452244 -2.35137526 -3.04452244\n",
      " -3.04452244 -1.94591015 -3.04452244 -3.04452244 -3.04452244 -1.94591015\n",
      " -3.04452244 -3.04452244 -3.04452244 -3.04452244 -2.35137526 -2.35137526\n",
      " -2.35137526 -2.35137526]\n",
      "32\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs=len(trainMatrix) # 一共有多少个留言\n",
    "    numWords=len(trainMatrix[0]) # 每个留言有多少个单词\n",
    "    pAbusive=sum(trainCategory)/float(numTrainDocs) # p(c1)的概率\n",
    "    p0Num=np.ones(numWords)\n",
    "    p1Num=np.ones(numWords)\n",
    "    p0Denom=2.0\n",
    "    p1Denom=2.0\n",
    "    for i in range(numTrainDocs): # 遍历所有留言\n",
    "        #print(\"i:%d numTrainDocs: %s,trainMatrix:%s\" %(i,numTrainDocs,trainMatrix[i]))\n",
    "        if trainCategory[i]==1: # 这条留言是侮辱性留言\n",
    "            p1Num+=trainMatrix[i]\n",
    "            p1Denom+=sum(trainMatrix[i]) # 这个是存的当前留言的单词数\n",
    "        else:\n",
    "            p0Num+=trainMatrix[i]\n",
    "            p0Denom+=sum(trainMatrix[i])\n",
    "    \n",
    "    p1Vect=np.log(p1Num/p1Denom)\n",
    "    p0Vect=np.log(p0Num/p0Denom)\n",
    "    #print(\"p1Denom\",p1Denom)\n",
    "    #print(\"p1Num\",p1Num)\n",
    "    return p0Vect,p1Vect,pAbusive\n",
    "\n",
    "trainM=[]\n",
    "for postingListItem in postingList:\n",
    "    trainM.append(setOfWords2Vec(VocabOfpostingList,postingListItem))\n",
    "print(trainM) # 用数字描述单词的形式.\n",
    "\n",
    "p0v,p1v,pA=trainNB0(trainM,classVec)\n",
    "print(p0v)\n",
    "print(p1v)\n",
    "print(len(p1v))\n",
    "print(pA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classfied as %d 0\n",
      "classfied as %d 1\n"
     ]
    }
   ],
   "source": [
    "def classifyNB(vec2Classify,p0vec,p1vec,pClass1):\n",
    "    p1=sum(vec2Classify*p1vec)+np.log(pClass1) # 用log把乘法转成加法,这样可以更好地避免某个特征概率为0导致对所有p(wi|c)相乘的影响.\n",
    "    p0=sum(vec2Classify*p0vec)+np.log(1.0-pClass1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def bagOfWords2VecMN(vocabList,inputSet):\n",
    "    returnVec=[0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]+=1\n",
    "    return returnVec\n",
    "\n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0v,p1v,pAb=trainNB0(np.array(trainMat),np.array(listClasses))\n",
    "    testEntry=['love', 'my', 'dalmation'] # 看这个是否是\"侮辱性\"留言\n",
    "    thisDoc=np.array(setOfWords2Vec(vocabList=myVocabList,inputSet=testEntry))\n",
    "    ret=classifyNB(thisDoc,p0v,p1v,pAb)\n",
    "    print(\"classfied as %d\",ret)\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc=np.array(setOfWords2Vec(vocabList=myVocabList,inputSet=testEntry))\n",
    "    ret=classifyNB(thisDoc,p0v,p1v,pAb)\n",
    "    print(\"classfied as %d\",ret)\n",
    "\n",
    "    \n",
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用朴素贝叶斯对垃圾邮件进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate is: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens=re.split(r'\\W*',bigString)\n",
    "    return [token.lower() for token in listOfTokens if len(token)>2] # 先把邮件分割,然后返回大于2个字节的单词,同时也把标点符号取出掉了.了\n",
    "\n",
    "def spamMail():\n",
    "    docList=[]\n",
    "    classList=[]\n",
    "    fullText=[]\n",
    "    for i in range(1,26): # 生成词\n",
    "        wordlist =textParse(open('email/spam/%d.txt'%i,encoding='iso-8859-1').read())\n",
    "        docList.append(wordlist)\n",
    "        fullText.extend(wordlist)\n",
    "        classList.append(1)\n",
    "        wordlist =textParse(open('email/ham/%d.txt'%i,encoding='iso-8859-1').read())\n",
    "        docList.append(wordlist)\n",
    "        fullText.extend(wordlist)\n",
    "        classList.append(0)\n",
    "    vocabList=createVocabList(docList)\n",
    "    trainingSet=list(range(50)) # 参考 https://blog.csdn.net/qq_18433441/article/details/54835583\n",
    "    testSet=[]\n",
    "    for i in range(10):\n",
    "        randIndex=int(np.random.uniform(0,len(trainingSet))) # 在训练集中随机找10个做测试集.\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat=[]\n",
    "    train_labels=[]\n",
    "    for docindex in trainingSet:\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList,docList[docindex]))\n",
    "        train_labels.append(classList[docindex])\n",
    "    p0v,p1v,pSam=trainNB0(trainMat,train_labels)\n",
    "    errorCount=0\n",
    "    for testdocindex in testSet:\n",
    "        wordVect=bagOfWords2VecMN(vocabList,docList[testdocindex])\n",
    "        if classifyNB(wordVect,p0v,p1v,pSam)!=classList[testdocindex]:\n",
    "            errorCount+=1\n",
    "            print(\"cls error \",docList(testdocindex))\n",
    "    print(\"error rate is:\",float(errorCount/len(testSet)))\n",
    "spamMail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
