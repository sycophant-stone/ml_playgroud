{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree\n",
    "\n",
    "### dataset\n",
    "\n",
    "| fe1 | fe2 | fish|\n",
    "|-----|----|----   |\n",
    "| 1 |1      |'yes' |\n",
    "| 1 |1      |'yes' |\n",
    "| 1 |0      |'no'  |\n",
    "| 0 |1      |'no'  |\n",
    "| 0 |1      |'no'  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step1 create dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DateSet():\n",
    "    dataSet=[[1,1,'yes'],\n",
    "             [1,1,'yes'],\n",
    "             [1,0,'no'],\n",
    "             [0,1,'no'],\n",
    "             [0,1,'no'],]\n",
    "    featureNames=['no surfacing','flippers']\n",
    "    return dataSet,featureNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2 cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_crossEntropy(dataSet):\n",
    "    totalNum=len(dataSet)\n",
    "    label_count={} # 这里是大括号注意\n",
    "    for sample in dataSet:\n",
    "        curlabel=sample[-1]\n",
    "        if curlabel not in label_count.keys():\n",
    "            label_count[curlabel]=0 # 第一次初始化\n",
    "        label_count[curlabel]+=1\n",
    "        \n",
    "    crossEntropy=0.0\n",
    "    for key in label_count:\n",
    "        prob=float(label_count[key])/totalNum\n",
    "        crossEntropy-=prob*math.log(prob,2)\n",
    "    \n",
    "    return crossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step3 split dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataSet_with_axis(dataSet,tar_feature,exp_val):\n",
    "    \"\"\"把某个feature的值是exp_val的那个值从数据集中扣除\n",
    "    \n",
    "    args:\n",
    "        tar_feature: 第几个feature\n",
    "        exp_val: 该feature的期望值\n",
    "    \n",
    "    return:\n",
    "        扣除之后的数据集\n",
    "    \"\"\"\n",
    "    reduced_dataSet=[]\n",
    "    for sample in dataSet:\n",
    "        if sample[tar_feature]==exp_val:\n",
    "            Temp_reduced_dataSet=sample[:tar_feature] # 左端点\n",
    "            Temp_reduced_dataSet.extend(sample[tar_feature+1:]) # 右端点\n",
    "            reduced_dataSet.append(Temp_reduced_dataSet)\n",
    "    return reduced_dataSet\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4 calc info Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bset_feature_to_split(dataSet):\n",
    "    \"\"\"找到一个特征,能使该树的信息增益下降最快\n",
    "    args:\n",
    "        dataSet\n",
    "    \n",
    "    return:\n",
    "        下降最快的feature\n",
    "    \"\"\"\n",
    "    num_feature=len(dataSet[0])-1\n",
    "    num_sample=len(dataSet)\n",
    "    best_info_gain=0.0\n",
    "    best_feature=-1\n",
    "    base_entropy=calcula_crossEntropy(dataSet)\n",
    "    for i in range(num_feature): # 对于每个feature\n",
    "        cur_feature_list=[sample[i] for sample in dataSet] # 取出第i个feature\n",
    "        unique_feature_val=set(cur_feature_list)\n",
    "        new_info_gain=0.0\n",
    "        for feature_val in (unique_feature_val):\n",
    "            # H(D|A)\n",
    "            reduced_dataSet=split_dataSet_with_axis(dataSet,i,feature_val)\n",
    "            cur_sample_proportion=len(reduced_dataSet)/float(num_sample) # 子树Ni/N 比例\n",
    "            new_info_gain+=cur_sample_proportion*calcula_crossEntropy(reduced_dataSet)\n",
    "        info_gain=base_entropy-new_info_gain # 注意这里的info_gain的算法,需要知道基础gain.看那个减小的最大\n",
    "        if(info_gain>best_info_gain):\n",
    "            best_info_gain=info_gain\n",
    "            best_feature=i\n",
    "    \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5 create decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_label(labelList):\n",
    "    \"\"\"对labelList做统计,返回最高的label.\n",
    "    \n",
    "    \"\"\"\n",
    "    count_labels={}\n",
    "    for vote in labelList:\n",
    "        if vote not in labelList.keys(): # 每种label,第一次进来都要做初始化\n",
    "            count_labels[vote]=0\n",
    "        count_labels[vote]+=1\n",
    "    sorted_coutn_labels=sorted(labelList.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sorted_coutn_labels[0][0]\n",
    "\n",
    "def create_tree(dataSet,featureName):\n",
    "    \"\"\"根据信息增益递归地寻找子树\n",
    "    args:\n",
    "        featureName:对应的feature是0,1的数字,其实它是有含义的,这里需要对应的含义以便创建出的decision tree有意义\n",
    "    \n",
    "    return:\n",
    "        返回当前树的最大信息增益的子树\n",
    "    \"\"\"\n",
    "    labelList=[sample[-1] for sample in dataSet]\n",
    "    # 递归结束条件\n",
    "    if labelList.count(labelList[0])==len(labelList):\n",
    "        return labelList[0]\n",
    "    \n",
    "    if len(dataSet[0])==1: # dataSet 含有features和最后一列的label,如果split到只剩下最后一列的label,则不需再进行分割.\n",
    "        return majority_label(labelList)\n",
    "    \n",
    "    # 创建树\n",
    "    best_feature=find_bset_feature_to_split(dataSet)\n",
    "    best_feature_name=featureName[best_feature]\n",
    "    myTree={best_feature_name:{}}\n",
    "    del(featureName[best_feature])\n",
    "    feat_values=[sample[best_feature] for sample in dataSet]\n",
    "    unique_value=set(feat_values)\n",
    "    for val in unique_value:\n",
    "        sub_feature_name=featureName[:]\n",
    "        myTree[best_feature_name][val]=create_tree(split_dataSet_with_axis(dataSet,best_feature,val),sub_feature_name)\n",
    "    print(myTree)\n",
    "    return myTree\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flippers': {0: 'no', 1: 'yes'}}\n",
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n",
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n"
     ]
    }
   ],
   "source": [
    "dataSet,feature_names=create_DateSet()\n",
    "tree=create_tree(dataSet,feature_names)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step7 classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(inputTree,featNames,testVec):\n",
    "    \"\"\"根据已知决策树,输入特定的测试向量及对应的feature名字,得到该测试向量是否属于期望label\n",
    "    args:\n",
    "        inputTree: 已经生成的决策树 \n",
    "        featNames: 特征名字集合\n",
    "        testVec: 待分类向量\n",
    "    \"\"\"\n",
    "    first_str=list(inputTree.keys())[0]\n",
    "    second_dict=inputTree[first_str]\n",
    "    feature_index=featNames.index(first_str)\n",
    "    key=testVec[feature_index]\n",
    "    value_of_feature=second_dict[key]\n",
    "    if isinstance(value_of_feature,dict):\n",
    "        label_list=classify(value_of_feature,featNames,testVec)\n",
    "    else:\n",
    "        label_list=value_of_feature\n",
    "    return label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "no\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "dataSet,feature_names=create_DateSet()\n",
    "print(classify(tree,feature_names,[1,0]))\n",
    "print(classify(tree,feature_names,[0,1]))\n",
    "print(classify(tree,feature_names,[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
